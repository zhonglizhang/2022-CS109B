{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003cimg style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"\u003e CS109B Data Science 2: Advanced Topics in Data Science \n",
                "\n",
                "## Lab 02 - Bayesian Analysis \n",
                "\n",
                "**Harvard University**\u003cbr\u003e\n",
                "**Spring 2022**\u003cbr\u003e\n",
                "**Instructors:** Mark Glickman and Pavlos Protopapas\u003cbr\u003e\n",
                "**Lab instructor and content:** Eleni Angelaki Kaxiras\u003cbr\u003e\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import warnings\n",
                "\n",
                "import arviz as az\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pymc3 as pm\n",
                "import theano.tensor as tt\n",
                "\n",
                "from pymc3 import summary\n",
                "\n",
                "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib import gridspec\n",
                "import scipy.stats as stats\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "%matplotlib inline \n",
                "\n",
                "import warnings\n",
                "print('Running on PyMC3 v{}'.format(pm.__version__))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%javascript\n",
                "IPython.OutputArea.auto_scroll_threshold = 20000;"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "#pandas trick\n",
                "pd.options.display.max_columns = 50  # None -\u003e No Restrictions\n",
                "pd.options.display.max_rows = 200    # None -\u003e Be careful with this \n",
                "pd.options.display.max_colwidth = 100\n",
                "pd.options.display.precision = 3"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Learning Objectives\n",
                "\n",
                "By the end of this lab, you should be able to:\n",
                "* Define a probabilistic model in the PyMC3 framework\n",
                "* Run a sampling algorithm \n",
                "\n",
                "**This lab corresponds to Lectures 3,4, and maps to Homework 2.**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Bayes Rule\n",
                "\n",
                "We have data that we believe come from an underlying distribution of unknown parameters. If we find those parameters, we know everything about the process that generated this data and we can make inferences (create new data).\n",
                "\n",
                "\\begin{equation}\n",
                "\\label{eq:bayes} \n",
                "P(\\theta|\\textbf{D}) = \\frac{P(\\textbf{D} |\\theta) P(\\theta) }{P(\\textbf{D})} \n",
                "\\end{equation}\n",
                "\n",
                "$P(\\theta|\\textbf{D})$ is the **posterior** distribution, prob(hypothesis | data) \n",
                "\n",
                "$P(\\textbf{D} |\\theta)$ is the **likelihood** function, how probable is my data **B** for different values of the parameters\n",
                "\n",
                "$P(\\theta)$ is the marginal probability to observe the data, called the **prior**, this captures our belief about the data before observing it.\n",
                "\n",
                "$P(\\textbf{D})$ is the marginal distribution (sometimes called marginal likelihood)\n",
                "\n",
                "#### But what is $\\theta \\;$?\n",
                "\n",
                "$\\theta$ is an unknown yet fixed set of parameters. In Bayesian inference we express our belief about what $\\theta$ might be and instead of trying to guess $\\theta$ exactly, we look for its **probability distribution**. What that means is that we are looking for the **parameters** of that distribution. For example, for a Poisson distribution our $\\theta$ is only $\\lambda$. In a normal distribution, our $\\theta$ is often just $\\mu$ and $\\sigma$."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id=pymc3\u003e\u003c/a\u003e [Top](#top)\n",
                "\n",
                "## 2. Introduction to `pyMC3`\n",
                " \n",
                "PyMC3 is a Python library for programming Bayesian analysis, and more specifically, data creation, model definition, model fitting, and posterior analysis. It uses the concept of a `model` which contains assigned parametric statistical distributions to unknown quantities in the model. Within models we define random variables and their distributions. A distribution requires at least a `name` argument, and other `parameters` that define it. You may also use the `logp()` method in the model to build the model log-likelihood function. We define and fit the model.\n",
                "\n",
                "PyMC3 includes a comprehensive set of pre-defined statistical distributions that can be used as model building blocks. Although they are not meant to be used outside of a `model`, you can invoke them by using the prefix `pm`, as in `pm.Normal`. To use them outside of a model one needs to invoke them as `pm.Normal.dist()`."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Probability distributions in `scipy` and `PyMC3`\n",
                "\n",
                "We can invoke probability distributions from `scipy` or directly from `PyMC3`. Distributions in `PyMC3` live within the context of models, although the framework provides a way to use the distributions outside of models. For a review of most common discete and continuous distributions see separate notebook.\n",
                "\n",
                "### `scipy`\n",
                " \n",
                "- **Normal** (a.k.a. Gaussian)\n",
                "\\begin{equation}\n",
                "X \\sim  \\mathcal{N}(\\mu,\\,\\sigma^{2})\n",
                "\\end{equation} \n",
                "\n",
                "    A Normal distribution can be parameterized either in terms of precision $\\tau$ or variance $\\sigma^{2}$. The link between the two is given by\n",
                "\\begin{equation}\n",
                "\\tau = \\frac{1}{\\sigma^{2}}\n",
                "\\end{equation}\n",
                " - Expected value (mean) $\\mu$\n",
                " - Variance $\\frac{1}{\\tau}$ or $\\sigma^{2}$\n",
                " - Parameters: `mu: float`, `sigma: float` or `tau: float`\n",
                " - Range of values (-$\\infty$, $\\infty$)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.style.use('seaborn-darkgrid')\n",
                "x = np.linspace(-5, 5, 1000)\n",
                "mus = [0., 0., 0., -2.]\n",
                "sigmas = [0.4, 1., 2., 0.4]\n",
                "for mu, sigma in zip(mus, sigmas):\n",
                "    pdf = stats.norm.pdf(x, mu, sigma)\n",
                "    plt.plot(x, pdf, label=r'$\\mu$ = '+ f'{mu},' + r'$\\sigma$ = ' + f'{sigma}') \n",
                "plt.xlabel('random variable', fontsize=12)\n",
                "plt.ylabel('probability density', fontsize=12)\n",
                "plt.legend(loc=1)\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### `PyMC3`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "az.style.use(\"arviz-darkgrid\")\n",
                "\n",
                "a = pm.Poisson.dist(mu=4)\n",
                "b = pm.Normal.dist(mu=0, sigma=10)\n",
                "\n",
                "_, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
                "az.plot_dist(a.random(size=1000), color=\"C1\", label=\"Poisson\", ax=ax[0])\n",
                "az.plot_dist(b.random(size=1000), color=\"C2\", label=\"Gaussian\", ax=ax[1])\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- [Distributions in PyMC3](https://docs.pymc.io/api/distributions.html)\n",
                "\n",
                "Information about PyMC3 functions including descriptions of distributions, sampling methods, and other functions, is available via the `help` command."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "## uncomment to look at the documentation \n",
                "#help(pm.Poisson)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id=blr\u003e\u003c/a\u003e [Top](#top)\n",
                "\n",
                "## 3. Bayesian Linear Regression"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Define the Problem"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Our problem is the following: we want to perform multiple linear regression to predict an outcome variable $Y$ which depends on variables $\\bf{x}_1$ and $\\bf{x}_2$.\n",
                "\n",
                "We will model $Y$ as normally distributed observations with an expected value $mu$ that is a linear function of the two predictor variables, $\\bf{x}_1$ and $\\bf{x}_2$.\n",
                "\n",
                "\\begin{equation}\n",
                "Y \\sim  \\mathcal{N}(\\mu,\\,\\sigma^{2})\n",
                "\\end{equation} \n",
                "\n",
                "\\begin{equation}\n",
                "\\mu = \\beta_0 + \\beta_1 \\bf{x}_1 + \\beta_2 x_2 \n",
                "\\end{equation}\n",
                "\n",
                "where $\\sigma^2$ represents the measurement error (in this example, we will use $\\sigma = 10$). **Note:** In the code we give the value for the standard deviation $\\sigma$.\n",
                "\n",
                "We also choose the parameters to have normal distributions with those parameters set by us.\n",
                "\n",
                "\\begin{eqnarray}\n",
                "\\beta_i \\sim  \\mathcal{N}(0,\\,10) \\\\\n",
                "\\sigma^2 \\sim  |\\mathcal{N}(0,\\,10)|\n",
                "\\end{eqnarray}   \n",
                "We will artificially create the data to predict on. We will then see if our model predicts them correctly."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Artificially create some data to test our model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(123)\n",
                "\n",
                "# True parameter values \u003c --- our model does not see these\n",
                "sigma = 1\n",
                "beta0 = 1\n",
                "beta = [1, 2.5]  \n",
                "\n",
                "# Size of dataset\n",
                "size = 100\n",
                "\n",
                "# Predictor variable\n",
                "x1 = np.linspace(0, 1., size)\n",
                "x2 = np.linspace(0,2., size)\n",
                "\n",
                "# Simulate outcome variable\n",
                "Y = beta0 + beta[0]*x1 + beta[1]*x2 + np.random.randn(size)*sigma"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "fig = plt.figure(figsize=(5,4))\n",
                "fontsize=14\n",
                "labelsize=8\n",
                "title='Observed Data ' + r'$Y(x_1,x_2)$' + ' (created artificially)'\n",
                "ax = fig.add_subplot(111, projection='3d')\n",
                "\n",
                "ax.scatter(x1, x2, Y)\n",
                "ax.set_xlabel(r'$x_1$', fontsize=fontsize)\n",
                "ax.set_ylabel(r'$x_2$', fontsize=fontsize)\n",
                "ax.set_zlabel(r'$Y$', fontsize=fontsize)\n",
                "\n",
                "ax.tick_params(labelsize=labelsize)\n",
                "\n",
                "fig.suptitle(title, fontsize=fontsize)        \n",
                "fig.tight_layout(pad=.1, w_pad=10.1, h_pad=2.)\n",
                "#fig.subplots_adjust(); #top=0.5\n",
                "plt.tight_layout\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's see if our model will correctly predict the values for our unknown parameters, namely $b_0$, $b_1$, $b_2$ and $\\sigma$."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Define the Model in PyMC3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pymc3 import Model, Normal, HalfNormal, model_to_graphviz\n",
                "from pymc3 import NUTS, sample, find_MAP\n",
                "from scipy import optimize"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Step1:** Formulate the probability model for our data: $Y \\sim  \\mathcal{N}(\\mu,\\,\\sigma^{2})$. This the **likelihood function** and it's defined the same as the other distributions except that there is an `observed` argument which means its values are determined by the data.\n",
                "\n",
                "```\n",
                "Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=Y)\n",
                "```\n",
                "\n",
                "**Step2:** Choose a prior distribution for our unknown parameters. \n",
                "```\n",
                " beta0 = Normal('beta0', mu=0, sigma=10)\n",
                " \n",
                " # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2)\n",
                " # so, in array notation, our beta1 = betas[0], and beta2=betas[1]\n",
                " betas = Normal('betas', mu=0, sigma=10, shape=2) \n",
                " \n",
                " sigma = HalfNormal('sigma', sigma=1)\n",
                "\n",
                "```\n",
                "\n",
                "**Step3:** Determine the posterior distribution, **this is our main goal**.\n",
                "\n",
                "**Step4:** Summarize important features of the posterior and/or plot the parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "with Model() as my_linear_model:\n",
                "\n",
                "    # Priors for unknown model parameters, specifically created stochastic random variables \n",
                "    # with Normal prior distributions for the regression coefficients,\n",
                "    # and a half-normal distribution for the standard deviation of the observations.\n",
                "    # These are our parameters.\n",
                "    \n",
                "    beta0 = Normal('beta0', mu=0, sd=1)\n",
                "    # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2)\n",
                "    # so, in array notation, our beta1 = betas[0], and beta2=betas[1]\n",
                "    betas = Normal('betas', mu=0, sd=1, shape=2) \n",
                "    sigma = HalfNormal('sigma', sd=1)\n",
                "    \n",
                "    # mu is what is called a deterministic random variable, which implies that its value is completely\n",
                "    # determined by its parents’ values (betas and sigma in our case). \n",
                "    # There is no uncertainty in the variable beyond that which is inherent in the parents’ values\n",
                "    \n",
                "    mu = beta0 + betas[0]*x1 + betas[1]*x2\n",
                "    \n",
                "    # Likelihood function = how probable is my observed data?\n",
                "    # This is a special case of a stochastic variable that we call an observed stochastic.\n",
                "    # It is identical to a standard stochastic, except that its observed argument, \n",
                "    # which passes the data to the variable, indicates that the values for this variable were observed, \n",
                "    # and should not be changed by any fitting algorithm applied to the model. \n",
                "    # The data can be passed in the form of either a numpy.ndarray or pandas.DataFrame object.\n",
                "    \n",
                "    Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note: If our problem was a classification for which we would use Logistic regression see [below](#LR) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_to_graphviz(my_linear_model)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Markov Chain Monte Carlo (MCMC) Simulations\n",
                "\n",
                "PyMC3 uses the **No-U-Turn Sampler (NUTS)** and the **Random Walk Metropolis**, two Markov chain Monte Carlo (MCMC) algorithms for sampling in posterior space. Monte Carlo gets into the name because when we sample in posterior space, we choose our next move via a pseudo-random process. NUTS is a sophisticated algorithm that can handle a large number of unknown (albeit continuous) variables."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Fitting the Model with Sampling - Doing Inference"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "See below for PyMC3's sampling method. As you can see it has quite a few parameters. Most of them are set to default values by the package. For some, it's useful to set your own values.\n",
                "```\n",
                "pymc3.sampling.sample(draws=500, step=None, init='auto', n_init=200000, start=None, trace=None, chain_idx=0, chains=None, cores=None, tune=500, progressbar=True, model=None, random_seed=None, discard_tuned_samples=True, compute_convergence_checks=True, **kwargs)\n",
                "```\n",
                "\n",
                "Parameters to set:\n",
                "\n",
                "- **draws** (int): number of samples to draw, defaults to 500. \n",
                "- **step** (MCMC method): implementation method for MCMC. Better to let pyMC3 assign the best one. If manually setting we can choose between some implementations such as `Metropolis()` or `NUTS()`.\n",
                "- **tune** (int): number of iterations to tune for, a.k.a. the \"burn-in\" period, defaults to 500. \n",
                "- **target_accept** (float in $[0, 1]$). The step size is tuned such that we approximate this acceptance rate. Higher values like 0.9 or 0.95 often work better for problematic posteriors.\n",
                "- (optional) **cores** (int) number of chains to run in parallel, defaults to the number of CPUs in the system, but at most 4.\n",
                "\n",
                "`pm.sample` returns a `pymc3.backends.base.MultiTrace` object that contains the samples. We usually name it `trace`. All the information about the posterior is in `trace`, which also provides statistics about the sampler."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "## uncomment this to see more about pm.sample\n",
                "#help(pm.sample)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Specify a `NUTS()` sampler\n",
                "\n",
                "It is the default and we expect good results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "with my_linear_model:\n",
                " \n",
                "    print(f'Starting MCMC process')\n",
                "    # draw 2000 posterior samples and run the default number of chains = 4 \n",
                "    trace = sample(2000, tune=1000, target_accept=0.9) \n",
                "    print(f'DONE')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "az.plot_posterior(trace, var_names=['beta0']);"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's compare with the true hidden parameter values \u003cbr\u003e\n",
                "sigma = 1, beta0 = 1 , beta = [1, 2.5] "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pymc3 import traceplot, compareplot, plot_posterior, forestplot\n",
                "traceplot(trace);"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "$\\hat{R}$ is a metric for comparing how well a chain has converged to the equilibrium distribution by comparing its behavior to other randomly initialized Markov chains. Multiple chains initialized from different initial conditions should give similar results.\n",
                "If all chains converge to the same equilibrium, $\\hat{R}$ will be 1. If the chains have not converged to a common distribution, $\\hat{R}$ will be \u003e 1.01. $\\hat{R}$ is a necessary but not sufficient condition.\n",
                "\n",
                "For details on the $\\hat{R}$ see *Gelman and Rubin (1992)*. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": [
                "pm.rhat(trace)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "forestplot(trace, varnames=['beta0', 'betas', 'sigma'], r_hat=True);"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Specify a `Metropolis()` sampler\n",
                "\n",
                "We do not expect good results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": [
                "#help(pm.backends.base.MultiTrace)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "with my_linear_model:\n",
                " \n",
                "    print(f'Starting MCMC process')\n",
                "    # draw 2000 posterior samples and run the default number of chains = 4 \n",
                "    trace = sample(2000, step=pm.Metropolis(), tune=1000) #, target_accept=0.9) \n",
                "    print(f'DONE')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pymc3 import traceplot, compareplot, plot_posterior, forestplot\n",
                "traceplot(trace);"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Model Plotting\n",
                "\n",
                "PyMC3 provides a variety of visualizations via plots: [https://docs.pymc.io/api/plots.html](https://docs.pymc.io/api/plots.html).\n",
                "\n",
                "One of them is the `traceplot`, another is the `compareplot`. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": [
                "az.plot_posterior(trace, var_names=['beta0']);"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#help(az.plot_posterior)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#help(pm.Normal)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Then we will generate and display our table\n",
                "az.summary(trace)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": [
                "trace.varnames"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#help(pm.backends.base.MultiTrace)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This linear regression example is from the original paper on PyMC3: *Salvatier J, Wiecki TV, Fonnesbeck C. 2016. Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 https://doi.org/10.7717/peerj-cs.55*"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id=LR\u003e\u003c/a\u003e\n",
                "\n",
                "### What about Logistic Regression?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If the problem above was a classification that required a Logistic Regression, we would use the logistic function ( where $\\beta_0$ is the intercept, and $\\beta_i$ (i=1, 2, 3) determines the shape of the logistic function).\n",
                "\n",
                "\\begin{equation}\n",
                "Pr(Y=1|X_1,X_2,X3) = {\\frac{1}{1 + exp^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3)}}}\n",
                "\\end{equation}\n",
                "\n",
                "Since both $\\beta_0$ and the $\\beta_i$s can be any possitive or negative number, we can model them as gaussian random variables.\n",
                "\n",
                "\\begin{eqnarray}\n",
                "\\beta_0 \\sim  \\mathcal{N}(\\mu,\\,\\sigma^2) \\\\\n",
                "\\beta_i \\sim  \\mathcal{N}(\\mu_i,\\,\\sigma_i^2)\n",
                "\\end{eqnarray} \n",
                "\n",
                "In PyMC3 we can model those as:\n",
                "```\n",
                "pm.Normal('beta_0', mu=0, sigma=100)\n",
                "```\n",
                "(where $\\mu$ and $\\sigma^2$ can have some initial values that we assign them, e.g. 0 and 100)\n",
                "\n",
                "The dererministic variable would be:\n",
                "```\n",
                "p-logit = beta0 + beta_1 * X_1 + beta_2 * X_2 + beta_3 * X_3\n",
                "```\n",
                "To connect this variable (p_logit) with our observed data, we would use a Bernoulli as our likelihood.\n",
                "```\n",
                "our_likelihood = pm.Bernoulli('our_likelihood', logit_p=p-logit, observed=our_data)\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Notice that the main difference with Linear Regression is the use of a Bernoulli distribution instead of a Gaussian distribution, and the use of the logistic function instead of the\n",
                "identity function.\n",
                "\n",
                "We could also use `pm.Deterministic` as follows:\n",
                "```\n",
                "p_i = pm.Deterministic('p_i', pm.math.invlogit(beta0 + beta_1 * X_1 + beta_2 * X_2 + beta_3 * X_3)\n",
                "```\n",
                "And then add this to the likelihood function with the parameter `p` (p: float\n",
                "Probability of success (0 \u003c p \u003c 1), instead of `logit_p` (logit_p: float\n",
                "Logit of success probability). Note that only one of `p` or `logit_p` can be specified.\n",
                "Then you could define the likelihood:\n",
                "\n",
                "```\n",
                "likelh = pm.Bernoulli('likelh', p=p_i, observed=data)\n",
                "\n",
                "```\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {},
            "outputs": [],
            "source": [
                "# A reminder of what the logistic function looks like. \n",
                "# Play with parameters a and b to see the shape of the curve change\n",
                "b = 5.\n",
                "x = np.linspace(-8, 8, 100)\n",
                "plt.plot(x, 1 / (1 + np.exp(-b*x)))\n",
                "plt.xlabel('y')\n",
                "plt.ylabel('y=logistic(x)')"
            ]
        }
    ]
}
