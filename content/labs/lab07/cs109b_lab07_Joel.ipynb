{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b79244-bdb2-4de0-96bb-76c92737d99b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Introduction to Data Science\n",
    "\n",
    "## Lab 7:  Convolutional Neural Networks\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2022**<br/>\n",
    "**Instructors**: Mark Glickman & Pavlos Protopapas<br/>\n",
    "**Lab Team**: Eleni Kaxiras, Marios Mattheakis, Chris Gumb, and Shivas Jayaram<br/>\n",
    "**Authors**: Cedric Flamant, Chris Gumb, Hayden Joy, Eleni Kaxiras, and Pavlos Protopapas\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spoken-transaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.discussion {\n",
       "\tbackground-color: #ccffcc;\n",
       "\tborder-color: #88E97A;\n",
       "\tborder-left: 5px solid #0A8000; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-chambers",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this section, you should understand how images are represented in Python, have a strong understanding of the 'convolution' operation performed by CNNs, and be able to construct, train, and evaluate a CNN using Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-eleven",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook Contents\n",
    "- [**Using SEAS Jupyter Hub**](#jubyterhub) {3 min}\n",
    "- [**Working with Images in Python**](#images) {10 min)\n",
    "    - [Image as Tensors](#tensors)\n",
    "    - [An Image from Scratch](#imgfromscratch)\n",
    "    - [Preprocessing: Padding](#pad)\n",
    "    - [Preprocessing: Normalization](#norm) \n",
    "- [**An Implementation of \"Convolution\"**](#conv) {20 min}\n",
    "    - [Setting Convolution Parameters](#params)\n",
    "    - [Defining Filters: Kernels and Biases](#kernels)\n",
    "    - [Calculating Output Dimensions](#outputdim)\n",
    "    - [Convolution Implementation](#convloop)\n",
    "    - [Multiple Convolution Layers](#mulilayer)\n",
    "- [**Tensorflow Datasets**](#tfdatasets) {15 min}\n",
    "    - [Loading Datasets](#loadds)\n",
    "    - [The Dataset Object](#dsobj)\n",
    "    - [Take, Cardinality, & Batch](#take)\n",
    "    - [Cache, Prefetch, & Shuffle](#cache)\n",
    "    - [Preprocessing with Datasets](#dspreproc)\n",
    "    - [Data Augmentation](#dataaug)\n",
    "- [**Keras**](#keras) {25 min}\n",
    "    - [Layers of a CNN in Keras](#keras_layers)\n",
    "    - [**Excercise:** Defining a Model](#keras_model)\n",
    "    - [Training](#keras_train)\n",
    "    - [Plotting Training History](#plottinghistory)\n",
    "    - [Evaluating](#plottinghistory)\n",
    "    - [Callbacks](#callbacks)\n",
    "    - [**Exercise:** Improving on Baseline Model](#improving)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-transcript",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='jupyterhub'></a>\n",
    "## Using SEAS JupyterHub-GPU [^](#contents \"Back to Contents\")\n",
    "\n",
    "\n",
    "<img src='fig/need-a-gpu.jpg' width='250px'>\n",
    "\n",
    "**PLEASE READ**: [Instructions for Using SEAS JupyterHub](https://canvas.harvard.edu/courses/102556/pages/instructions-for-using-seas-jupyterhub)\n",
    "\n",
    "SEAS and FAS are providing you with a Jupyter computing environment to use for your CS109B course work. It is accessible from 'JupyterHub-GPU' in the menu on the Canvas course page. The **GPU** available on these instances allow for much faster NN training. The libraries defined in <a href='https://canvas.harvard.edu/files/14521374/download?download_frd=1'>cs109b.yml</a> (keras, tensorflow, pandas, etc.) are all pre-installed.\n",
    "\n",
    "**NOTE : This service funded by SEAS and FAS for the purposes of the class.**\n",
    "\n",
    "**NOTE NOTE NOTE: You are only to use JupyterHub-GPU for purposes directly related to CS109B coursework.**\n",
    "\n",
    "**Help us keep this service: Make sure you stop your instance as soon as you do not need it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3177b704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !conda install -c conda-forge ipywidgets --yes\n",
    "!conda install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "701e39d9-f849-4187-a8cf-4b639d52985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sys.excepthook:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda3\\envs\\cs109b\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1979, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'RuntimeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda3\\envs\\cs109b\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1981, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"D:\\Anaconda3\\envs\\cs109b\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1105, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"D:\\Anaconda3\\envs\\cs109b\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 999, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"D:\\Anaconda3\\envs\\cs109b\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 851, in structured_traceback\n",
      "    assert etb is not None\n",
      "AssertionError\n",
      "\n",
      "Original exception was:\n",
      "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "SystemError: <built-in method __contains__ of dict object at 0x000002370FE25500> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# from scipy.signal import convolve2d, correlate2d\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# from sklearn.datasets import load_sample_image\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cs109b\\lib\\site-packages\\tensorflow\\__init__.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cs109b\\lib\\site-packages\\tensorflow\\python\\__init__.py:40\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cs109b\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m executor\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cs109b\\lib\\site-packages\\tensorflow\\python\\client\\pywrap_tf_session.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tf_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tf_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _TF_SetTarget\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tf_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _TF_SetConfig\n",
      "\u001b[1;31mImportError\u001b[0m: SystemError: <built-in method __contains__ of dict object at 0x000002370FE25500> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from scipy.signal import convolve2d, correlate2d\n",
    "# from sklearn.datasets import load_sample_image\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-chorus",
   "metadata": {},
   "source": [
    "<a id='images'></a>\n",
    "## Working with Images in Python [^](#contents \"Back to Contents\")\n",
    "\n",
    "Convolutional neural networks were developed for working with image data, having been inspiried by the study of biological vision\\[[1](https://www.sciencedirect.com/science/article/pii/S187705091631674X)\\]\\[[2](https://arxiv.org/pdf/2001.07092.pdf)\\]. Before we dive into CNNs themselves, we should first learn a bit about how images are represented digitally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-contest",
   "metadata": {},
   "source": [
    "<div class='exercise' id='tensors'><b>Images as Tensors</b></div></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-baptist",
   "metadata": {},
   "source": [
    "The `matplotlib.pyplot` module has several functions we can use to work with images in Python: [imread](https://matplotlib.org/3.3.4/api/_as_gen/matplotlib.pyplot.imread.html), [imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html), and [imsave](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imsave.html).<br>\n",
    "Let's begin by loading an example image using `imread` and displaying it with `imshow`.<br>\n",
    "(painting by [Mark Rothko](https://www.nga.gov/features/mark-rothko.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('data/rothko.jpg', )\n",
    "plt.axis('off')\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-replica",
   "metadata": {},
   "source": [
    "What more can we learn about this `img` object that we created by reading in our image file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Image Type:', type(img))\n",
    "print('Image Shape:', img.shape)\n",
    "print('Array Type:', img.dtype)\n",
    "print(f'Value Range: [{img.min()}, {img.max()}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-archives",
   "metadata": {},
   "source": [
    "The image is represented as a 3-dimensional numpy array, or **tensor**. The 1st dimension is height, the 2nd is width, and the 3rd is the **color channel**.\n",
    "\n",
    "Each element, or pixel, in the array is an 8-bit, unsigned integer. $2^8 = 256$ so a pixel can take on values from 0 and 255. <br />\n",
    "Because the images a numpy array we can use familiar Python indexing to specify certain sections.<br>\n",
    "Let's explore the image's color channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456781b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3)\n",
    "rgb_list = ['Reds', 'Greens', 'Blues']\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(img[:,:,i], cmap=rgb_list[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(rgb_list[i])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccfa28-8b9c-46a2-baf4-6f6ba21df6e8",
   "metadata": {},
   "source": [
    "When we pass a tensor with 3 channels to `imshow` it assumes these are <span style='color: red'>red</span>,  <span style='color: green'>green</span>, and  <span style='color: blue'>blue</span>.<br>\n",
    "However, for inputs with a different number of channels we need to use the [**cmap**](https://matplotlib.org/stable/tutorials/colors/colormaps.html) argument to specify the color gradient to map the pixel values onto. For example, there is nothing *intrinsically green* about the 2nd color channel plucked from our original image. It's just a 2-D array of integers where each integer represents the *intensity* of the pixel, but it contains no information about the color. If we want `imshow` to interpret it as green, or any other color, then we need to make that explicit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c8cc4a-3477-4b85-81a2-0b468ae842f2",
   "metadata": {},
   "source": [
    "**Note:** Some image formats such as png can have a 4th channel referred to as **alpha** which controls transparency. `imshow` knows how to handle these 4-channel images, but it is important to keep this in mind lest the extra channel catch you by surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-strip",
   "metadata": {},
   "source": [
    "<div class='exercise' id='imgfromscratch'><b>An Image from Scratch</b></div></br>\n",
    "\n",
    "We can use what we now know about an image's digital representation to build an image from scratch by creating and manipulating a `numpy` array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-latin",
   "metadata": {},
   "source": [
    "Let's create a 5x5 RGB image made from scratch using `numpy` and call it `toy_img`.\n",
    "* The <span style='color:red'>red</span> channel will contain a diagonal line starting in the upper right corner <span style='color:red'><strong>\\\\</strong></span>\n",
    "* The <span style='color:green'>green</span> channel will contain a 'backwards' diagonal line starting in the lower right corner <span style='color:green'>**/**</span>\n",
    "* The <span style='color:blue'>blue</span> channel will have a vertical line in the middle of the image <span style='color:blue'>**|**</span>\n",
    "\n",
    "To keep it simple, all pixels in each channel will be either fully on or off<br>\n",
    "We'll use `1` for all the 'on' pixels and `0` for 'off' pixels.<br>\n",
    "\n",
    "* `np.zeros()` can create an image of all 'off' pixels; we can then 'turn some on' using slicing/indexing\n",
    "* `np.eye(N)` creates an identity NxN matrix \n",
    "* `np.flipud(a)` flips the array `a` upsidedown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_img = np.zeros((5,5,3)) # blank image of all 'off' pixels\n",
    "# RED\n",
    "toy_img[:,:,0] = np.eye(5) # diagonal of red\n",
    "# GREEN\n",
    "toy_img[:,:,1] = np.flipud(np.eye(5)) # backwards diagonal of green\n",
    "# BLUE\n",
    "toy_img[:,2,2] = 1 # vertical line of blue in the middle\n",
    "plt.imshow(toy_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-motion",
   "metadata": {},
   "source": [
    "Looking good! Note how the center pixel where all 3 channels are set to `1` is white.<br> \n",
    "But if we are thinking of `toy_img` as a 3-channel image then something unintuitive happens when we try and display the image as a 3d numpy array (i.e., tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 5x5x3 toy image tensor\n",
    "toy_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-blast",
   "metadata": {},
   "source": [
    "What's going on here! We see 5 matrices, each 5x3. It is hard to tell by looking at this just what is going on in each color channel.<br>\n",
    "The problem is that numpy is splitting the display based on the first dimension (by row). You can think of this as treating the first dimension as though _that_ were the channel.<br>\n",
    "But, as long as we are thinking about this tensor as an an image with the last diemension specifying the color channel, we would prefer to split the display by this last dimension.<br>\n",
    "The `show_channels()` helper function does just that by swaping the first and last axes in the tensor and then printing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display image tensor split into color channels\n",
    "def show_channels(tensor):\n",
    "    channel_first= np.rollaxis(tensor, 2, 0)\n",
    "    return(channel_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display color channels as matrices\n",
    "show_channels(toy_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-shadow",
   "metadata": {},
   "source": [
    "Now we can clearly see our 3 color channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-gravity",
   "metadata": {},
   "source": [
    "<div class='exercise' id='pad'><b>Preprocessing: Padding</b></div></br>\n",
    "\n",
    "Most networks you'll encounter expect **inputs of a fixed dimension**. This can make life difficult if you want to train your CNN on a set of images of varying sizes. One solution would be to **crop** all images down to the same size, but this forces us to throw away what could be useful information.<br>\n",
    "A more palatable option is to **pad** our images by adding a border of zeros.<br>\n",
    "\n",
    "Previously, in CS109A, we learned how to add a column of ones to our NN's input design matrix with `np.ones()`. This allowed for multiplication with the bias terms during the forward pass. We could use a similar approach to add a padding of zeros around our image using `np.zeros` but that would be tedious! Luckily there is `np.pad()`:<br> [https://numpy.org/doc/stable/reference/generated/numpy.pad.html](https://numpy.org/doc/stable/reference/generated/numpy.pad.html)<br>\n",
    "But the use of this function can be a bit confusing at first, especially if you want to vary the amount of padding on each side of the image, or if you are working with higher dimensional tensors.<br>\n",
    "\n",
    "First, let's practice by creating a 3x3 matrix of ones and give it a padding of 2 on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "a = np.ones((3,3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded\n",
    "np.pad(a, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-scanning",
   "metadata": {},
   "source": [
    "Now for some more complicated padding:\n",
    "* 1 padding on top\n",
    "* 2 padding at bottom\n",
    "* 3 padding on left\n",
    "* 4 padding on right\n",
    "\n",
    "Display the resulting matrix\n",
    "\n",
    "**Note:** Don't forget to look at the `np.pad()` documentation, **either with `Shift+Tab` in Jupyter**, or by following link above.\n",
    "The `pad_width` argument takes a $n$-tuple of 2-tuples, where $n$ is the number of dimensions and the 2-tuples describe how much padding to add to the beginning and end of that dimension.<br>\n",
    "So in this case our `pad_width` looks like `((top, bottom), (left, right))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding a 2-D array\n",
    "np.pad(a, pad_width=((1,2),(3,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-reasoning",
   "metadata": {},
   "source": [
    "Now we add the same padding above to `toy_img` and store the result in `toy_img_pad`.\n",
    "\n",
    "**Note:** Consider the dimensions of `toy_img`. We need another 2-tuple for the channel dimension, but we wouldn't want to pad *this* dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding a 3-D tensor\n",
    "toy_img_pad = np.pad(toy_img, ((1, 2), (3, 4), (0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the tensor by channel\n",
    "show_channels(toy_img_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display as an image\n",
    "plt.imshow(toy_img_pad);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-cargo",
   "metadata": {},
   "source": [
    "<div class='exercise' id='norm'><b>Preprocessing: Normalizing</b></div></br>\n",
    "\n",
    "If we're going to be using our images as input to a neural network we should normalize all pixel values to between 0 and 1. This helps our gradients 'well behaved.'\n",
    "\n",
    "We saw earlier that the pixels are unsigned 8-bit integers and so can take on values between 0 and 255. Now that we know the max possible value normalization is easy. We simply divide the tensor representing the image by 255!\n",
    "\n",
    "`imgshow()` is clever enough to display normalized images exaclty the same as it displays the unnormalized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_norm = img/255.\n",
    "fig, axs = plt.subplots(1,2)\n",
    "for ax, image, title in zip(axs, [img, img_norm], ['unnormalized', 'normalized']):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-truth",
   "metadata": {},
   "source": [
    "We can take what we've learned to make a 'forgery' of the original Rothko painting! 🤖🎨🖌️ 🖼️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our blank 'canvas' of black (i.e., zeros)\n",
    "rothko2 = np.zeros((8,5,3))\n",
    "# some red horizontal lines\n",
    "rothko2[:3,:,0] = 1\n",
    "# some green\n",
    "rothko2[4,:,:2] = 1\n",
    "# and some blue\n",
    "rothko2[6:,:,1:] = 1\n",
    "# add a black border\n",
    "rothko2 = np.pad(rothko2,((1,1),(1,1),(0,0)))\n",
    "# And voilà!\n",
    "plt.imshow(rothko2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-ottawa",
   "metadata": {},
   "source": [
    "Not too bad! And we see just what we'd expect when view the individual color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3)\n",
    "rgb_list = ['Reds','Greens','Blues']\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(rothko2[:,:,i], cmap=rgb_list[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(rgb_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_channels(rothko2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-above",
   "metadata": {},
   "source": [
    "Finally, let's save our work as a PNG for posterity with `imgsave` and ensure that we've done so correctly by reading and displaying it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave('data/rothko2.png', rothko2, vmin=0, vmax=1, format='PNG')\n",
    "test_load = plt.imread('data/rothko2.png')\n",
    "plt.imshow(test_load);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-outdoors",
   "metadata": {},
   "source": [
    "But what do we see if we inspect the representation of the loaded PNG? It's that tricky 4th \"color\" channel, alpha!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_channels(test_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-manhattan",
   "metadata": {},
   "source": [
    "<a id='conv'></a>\n",
    "## An Implementation of \"Convolution\" [^](#contents \"Back to Contents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-banks",
   "metadata": {},
   "source": [
    "<img src='fig/conv_op1.png' width='1000px'>\n",
    "\n",
    "A convolutional layer is composed of **filters** which are composed of **kernels** which are *themselves* composed of **weights**. Each filter also has a **bias** term though it is often not depicted in diagrams (it is exluded in the one above for example). We *learn* the weights and biases from our data. Each conv layer also has an associated **activation function** such as ReLU or sigmoid. \n",
    "\n",
    "In Keras, the **number of filters** and the **height and width of the kernels** of which they consist are set by the `filters` (int) and `kernel_size` (int or tuple) arguments respectively. \n",
    "\n",
    "The **depth of the filters is fixed** by the depth (i.e., number of 'channels' or 'filter maps') of the input to the conv layer. \n",
    "\n",
    "The output of the conv layer, if we have multiple filters, is a 3D tensor which is a set of **feature maps**. Each feature map is itself the output of one of the layer's filters \"convolving\" on the input. The height and width of the output feature map tensor is a function of the input size, `kernel_size`, `padding`, and `stride`. The depth of the output tensor (i.e, number of feature maps) is equal to the number of `filters` in the layer.\n",
    "\n",
    "The best way to build intuition for what the covolution layers in Keras are (roughly) doing is to step through a basic implementation of our own.<br>\n",
    "We'll construct a basic implementation and explore the affect of some different filters and parameters on the output.\n",
    "\n",
    "For this section we'll use this lovely photograph of a pagoda that comes with `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "# normalized image\n",
    "img = load_sample_image('china.jpg')/255.\n",
    "plt.axis('off')\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4163f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3)\n",
    "rgb_list = ['Reds','Greens','Blues']\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(img[:,:,i], cmap=rgb_list[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(rgb_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-scout",
   "metadata": {},
   "source": [
    "<div class='exercise' id='params'><b>Convolution Parameters</b></div></br>\n",
    "\n",
    "We'll need to set some basic parameters for our convolution.\n",
    "\n",
    "\n",
    "**Number of Filters**: determines the number of output feature maps \n",
    "\n",
    "**Kernel Dimensions**: height and width of the filters (depth determined by input depth)\n",
    "\n",
    "**Padding**: extra pixels added to the border of the input. Typically these are zeros. Common paddings settings are 'valid' ('valid' padding means no padding at all and differs from 'full' padding), full padding, which ensures every pixel of the input is visited the same number of times by the filter, and 'same' which ensures the output is the same size (height and width) as the input.\n",
    "\n",
    "<img src='fig/padding.png' width='500px'>\n",
    "\n",
    "**Stride**: how large of a 'step' the filter takes when convolving.\n",
    "\n",
    "<img src='fig/stride.png' width='400px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-remove",
   "metadata": {},
   "source": [
    "Almost anything will work here and we can come back to play with these values later. But let's start simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic convolution parameters\n",
    "n_filters = 3 # number of filters (each producing a feature map)\n",
    "k_dim = (3,3) # kernel height and width\n",
    "padding = 1 # zeros to add around border\n",
    "strides = 1 # size of step when convolving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-platform",
   "metadata": {},
   "source": [
    "<div class='exercise' id='kernels'><b>Defining Filters: Kernels and Biases</b></div></br>\n",
    "\n",
    "\n",
    "We will store the weights of our filters in a 4D tensor (which is also how Keras stores the weights in its Conv2D layers as we'll see later). We'll call our weight tensor `filters`.<br>\n",
    "The dimensions are: (number of filters) x (kernel height) x (kernel width) x (filter depth)<br>\n",
    "\n",
    "For our example, `filters` will contain 3 identical filters, each composed of 3 identical kernels, and the values in each filter will sum to 1.<br>\n",
    "\n",
    "Each filter also has a bias term. We'll create `biases` to be a numpy array of zeros (you can also try changing it later to see what affect it has)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters is a 4D tensor\n",
    "# Number of filters x kernel height x kernel width x filter depth\n",
    "filters = np.ones((n_filters, *k_dim, img.shape[-1]))/np.prod([*k_dim, img.shape[-1]])\n",
    "\n",
    "# biases is a 1d array; there is one bias for each filter\n",
    "biases = np.zeros(n_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-minute",
   "metadata": {},
   "source": [
    "**Q:** Why is it important that the weights in each kernel sum to one? What would happen if they are larger or smaller? 🤔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c63797-1b3e-47a4-9fd2-eada486c348e",
   "metadata": {},
   "source": [
    "**Q:** Why might we want 3 identical filters here? 🤔 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-valuable",
   "metadata": {},
   "source": [
    "<div class='exercise' id='outputdim'><b>Calculating Output Dimensions</b></div></br>\n",
    "\n",
    "Calculating the output dimensions based on our input image and parameters will be essential for implementing convolution. The function should throw an error if the dimensions are not integers.<br>\n",
    "We can refer to the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-perry",
   "metadata": {},
   "source": [
    "$$O = \\frac{W - K + 2P}{S} + 1$$\n",
    "Where $O$ is the output dim, $W$ is the input dim, $K$ is the filter size, $P$ is padding, and $S$ is the stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates output shape based on input and conv parameters\n",
    "def output_dim(img: np.array, filters: np.array, padding: int, strides: int):\n",
    "    w = np.array(img.shape[:-1])\n",
    "    k = filters[0].shape[:-1]\n",
    "    out_dim = (w - k + 2*padding)/strides + 1\n",
    "    assert (out_dim % 1).all() == 0, \"Calculated output dimensions not integer valued\"\n",
    "    return out_dim.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = output_dim(img, filters, padding, strides)\n",
    "out_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-oliver",
   "metadata": {},
   "source": [
    "<div class='exercise' id='convloop'><b>\"Convolution\"</b></div></br>\n",
    "<img src='fig/convolve.gif' width='200px'>\n",
    "Here we'll implement the \"convolution\" (as that term is used in CNNs) algorithm as a function.<br>\n",
    "\n",
    "We'll make use of `out_dim` and what we learned about `np.pad`. We won't implement 'valid' or 'same' padding, only a uniform amount of padding provided to all sides of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c391ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-commitment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convolution(img, filters, biases=None, padding=1, strides=1):\n",
    "    # pad input image\n",
    "    img = np.pad(img, ((padding, padding), (padding, padding), (0,0)))\n",
    "    # get new input dimensions after padding\n",
    "    input_dim = np.array(img.shape)\n",
    "    # determine output dimensions \n",
    "    filter_map_dim = output_dim(img, filters, padding, strides)\n",
    "    # find dimensions of individual filter\n",
    "    k_dim = filters.shape[1:]\n",
    "    # if no biases specified, set them to zero (num biases == num filters)\n",
    "    if biases is None:\n",
    "        biases = np.zeros(filters.shape[0])\n",
    "    \n",
    "    # create empty array to store the convolution results \n",
    "    filter_maps = np.ones(shape=(*filter_map_dim, len(filters)))\n",
    "    # loop over filters\n",
    "    for f in range(filters.shape[0]):\n",
    "        # init y pos of upper left corner of filter on input image to 0\n",
    "        input_y = filter_maps_y = 0\n",
    "        # loop over rows until we run off the bottom\n",
    "        while input_y + k_dim[0] <= input_dim[0]:\n",
    "            # init x pos of upper left corner of filter on input image to 0 \n",
    "            input_x = filter_maps_x = 0\n",
    "            # loop over columns until we run off the side\n",
    "            while input_x + k_dim[1] <= input_dim[1]:\n",
    "                # x & y coordinates of input that filter currently 'sees'\n",
    "                xs = slice(input_x, input_x + k_dim[0])\n",
    "                ys = slice(input_y, input_y + k_dim[1])\n",
    "                # elementwise multiplication, add bias, sum, and store in output\n",
    "                filter_maps[filter_maps_y, filter_maps_x, f] = np.sum(img[ys, xs, :] * filters[f]) + biases[f]\n",
    "                # advance column based on stride\n",
    "                input_x += strides\n",
    "                # NOTE: movement over output img NOT based on stride!\n",
    "                filter_maps_x += 1\n",
    "            # advance row based on stride\n",
    "            input_y += strides\n",
    "            # NOTE: movement over output img NOT based on stride!\n",
    "            filter_maps_y += 1\n",
    "    return filter_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bdd248-46e4-40b4-b7ca-366655b3ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out = convolution(img, filters, biases, padding, strides)\n",
    "\n",
    "def show_result(img, out):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(20,20))\n",
    "    out = np.clip(out, 0., 1.)\n",
    "    imgs = [img, out]\n",
    "    titles = ['original', 'output']\n",
    "    for i, title in enumerate(titles):\n",
    "        for spine in axs[i].spines.values():\n",
    "            spine.set_visible(False)\n",
    "        axs[i].imshow(imgs[i])\n",
    "        axs[i].set_title(title)\n",
    "\n",
    "show_result(img, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-render",
   "metadata": {},
   "source": [
    "We can experiment with different values for the stride and padding and seeing how this affects the output.<br>\n",
    "You can also go back and change `k_dim` and rerun all the following cells to see how larger/smaller kernels affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50236f7-7b66-487c-b85b-fa261c8a8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# CONFIGURE CONV HERE\n",
    "n_filters = 4 # number of filters (each producing a feature map)\n",
    "k_dim = (3,3) # kernel height and width\n",
    "padding = 1 # zeros to add around border\n",
    "strides = 1 # size of step when convolving\n",
    "\n",
    "filters = np.ones((n_filters, *k_dim, img.shape[-1]))/np.prod([*k_dim, img.shape[-1]])\n",
    "biases = np.zeros(n_filters)\n",
    "out = convolution(img, filters, biases, padding, strides)\n",
    "show_result(img, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-zoning",
   "metadata": {},
   "source": [
    "**Q:** Why is the resulting image black & white? 🤔 <br>\n",
    "**Hint:** Consider the operation that takes place when were are calculating each pixel value of the output image and think back to the white pixel in the `toy_image` we made earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-irish",
   "metadata": {},
   "source": [
    "For variety, we'll make a copy of `kernels` using `np.copy()` and save it as `new_kernels`. Scale down the values in each kernels in `new_kernels` by a different value for each kernel. Now rerun the convolution. How does this affect the output image? Is it still black & white? If not, what else is wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "new_filters = filters.copy()\n",
    "new_filters[0] *= 0.8\n",
    "new_filters[1] *= 0.5\n",
    "new_filters[2] *= 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = convolution(img, new_filters, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(img, out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-thriller",
   "metadata": {},
   "source": [
    "<div class='exercise'><b>Sharpen Filter & Polychromatic Output</b></div></br>\n",
    "\n",
    "Now we'll create a 3D filter from the provided 2D `sharp_kernel` that will only affect the red color channel and save it as `filter_R`.\n",
    "**Hint:** `np.pad()` and `np.reshape()` may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_kernel = np.array([[ 0.,-1., 0.],\n",
    "                         [-1., 5.,-1.],\n",
    "                         [ 0.,-1., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_R = np.pad(sharp_kernel.reshape(*sharp_kernel.shape,-1), ((0,0), (0,0), (0,2)))\n",
    "filter_R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_channels(filter_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-dakota",
   "metadata": {},
   "source": [
    "Again using `sharp_filter` as a starting point, use a loop to create a 4D tensor `sharp_filters` where each kernel only operates on one of the 3 color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_filters = np.zeros((n_filters, *sharp_kernel.shape, img.shape[-1]))\n",
    "for i in range(sharp_filters.shape[-1]):\n",
    "    sharp_filters[i,:,:,i] = sharp_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, color in enumerate(['red', 'green', 'blue']):\n",
    "    print(color)\n",
    "    print(show_channels(sharp_filters[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-creativity",
   "metadata": {},
   "source": [
    "Now run the convolution again using your new sharp filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sharp = convolution(img, sharp_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(img, out_sharp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-lobby",
   "metadata": {},
   "source": [
    "Later, on your own, you should also try experimenting with how altering parameters like kernel size and stride can affect the output image when using the sharpen filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-mitchell",
   "metadata": {},
   "source": [
    "<div class='exercise' id='mulilayer'><b>Multiple Convolution Layers</b></div></br>\n",
    "\n",
    "<img src='fig/cnn2.png' width='700px'>\n",
    "\n",
    "The true power of CNNs comes from their 'deep' architecture. That is, when we stack convolution operations one after the other with the output **feature maps** of one convolutional 'layer' serving as the input to the next convolutional layer.\n",
    "\n",
    "In this next example we'll do just that. Creating a **2 layer CNN** and inspecting the feature map we get at as output from the second layer.<br>\n",
    "\n",
    "If each layer can be though of as learning certain features of the image, then layers deeper in the network are learning more complex and abstract features: features of features! 🤯\n",
    "\n",
    "We'll use a picture of <a href=\"https://en.wikipedia.org/wiki/Widener_Library\">Harvard's Widener Library</a> as our input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-villa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = plt.imread(\"data/Widener_Library.jpg\")\n",
    "# normalize\n",
    "img = img/255.\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-transparency",
   "metadata": {},
   "source": [
    "### 1st CNN Layer\n",
    "\n",
    "<img src='fig/conv-many-filters.png' width='550px'>\n",
    "\n",
    "Our first CNN layer will consist of **2 3x3x3 filters**: one for **vertical edge detection** and another for **horizontal edge detection**.<br>Each filter is composed of 3 identical 3x3 kernels, one for each of the input image's color channels.<br>\n",
    "(see https://en.wikipedia.org/wiki/Kernel_(image_processing) for kernel examples)\n",
    "\n",
    "We apply each filter to the input image like a CNN would:\n",
    "1. Convolution \n",
    "2. Add Bias\n",
    "3. Activation Function\n",
    "\n",
    "We'll use the **ReLU activation function** where $\\textrm{ReLU}(x) = \\textrm{max}(0,x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.clip(x, a_min=0., a_max=None)\n",
    "\n",
    "x = np.linspace(-1,1)\n",
    "plt.plot(x, relu(x))\n",
    "plt.title('ReLU Activation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d, correlate2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-simpson",
   "metadata": {},
   "source": [
    "Let's use the **vertical edge detection** filter first, saving the resulting **feature map** as `vedges`.\n",
    "\n",
    "**Implementation Notes:**\n",
    "- The version of convolution we implemented in the previous section may served to illustrate the operation, but with its nested loop structure it is far from efficient. For the following section we'll use [scipy.signal.convolve2d](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html) for our convolution. \n",
    "- `mode='same'` pads the input to ensure the output has the same height and width.\n",
    "- Because all the kernels in the filter are identical, we can just define one kernel and convolve it over each color channel in turn.\n",
    "- We are adding a bias of 0 which has no effect. We only include it to replicate the operations of a CNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a Vertical Edge detection filter\n",
    "vedge_kernel = np.array( [[1, 0, -1],\n",
    "                          [2, 0, -2],\n",
    "                          [1, 0, -1]])/3.\n",
    "# to store our output feature map\n",
    "vedges = np.zeros_like(img)\n",
    "# elementwise multiplication\n",
    "for i in range(3):\n",
    "    # vedges[:,:,i] = convolve2d(img[:,:,i], vedge_kernel, mode='same')\n",
    "    vedges[:,:,i] = correlate2d(img[:,:,i], vedge_kernel, mode='same')\n",
    "# sum results across channels, producing a flat feature map\n",
    "vedges = vedges.sum(-1)\n",
    "# add a bias \n",
    "bias = 0\n",
    "vedges += bias\n",
    "# ReLu activation\n",
    "vedges = relu(vedges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-engineering",
   "metadata": {},
   "source": [
    "We then perform the same operation with the **horizontal edge filter** on `img`, saving the resulting **feature map** as `hedges`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a Horizontal Edge detection filter\n",
    "hedge_kernel = np.array( [[ 1, 2, 1],\n",
    "                          [ 0, 0, 0],\n",
    "                          [-1,-2,-1]])/3.\n",
    "hedges = np.zeros_like(img)\n",
    "for i in range(3):\n",
    "    # hedges[:,:,i] = convolve2d(img[:,:,i], hedge_kernel, mode='same')\n",
    "    hedges[:,:,i] = correlate2d(img[:,:,i], hedge_kernel, mode='same')\n",
    "hedges = hedges.sum(-1)\n",
    "bias = 0\n",
    "vedges += bias\n",
    "hedges = relu(hedges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-document",
   "metadata": {},
   "source": [
    "Finally we visualize the two feature maps.<br>\n",
    "We'll show them in black & white by setting `cmap='gray'` in our call to `imshow`. This is just convention; we could use any color map we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the 2 feature maps\n",
    "fontsize = 20\n",
    "f, ax = plt.subplots(1,2, figsize=(13,8))\n",
    "ax[0].imshow(np.clip(vedges,a_min=None, a_max=1.), cmap='gray')\n",
    "ax[0].set_title('Vertical Edge Detection', {'fontsize': fontsize})\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(np.clip(hedges, a_min=None, a_max=1.), cmap='gray')\n",
    "ax[1].set_title('Horizontal Edge Detection', {'fontsize': fontsize})\n",
    "ax[1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-worse",
   "metadata": {},
   "source": [
    "### 2nd CNN Layer\n",
    "Together, `vedges` and `hedges` could be the output of the first layer of a CNN with 2 filters. Now we will investigate what can happen when we stack convolution layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-marketplace",
   "metadata": {},
   "source": [
    "To achieve this, we'll concatenate `vedges` and `hedges` in a third dimension, calling the output `feature_maps`. `feature_maps` should have dimensions (267, 400, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the feature maps\n",
    "feature_maps = np.concatenate((np.expand_dims(vedges, axis=-1), np.expand_dims(hedges,axis=-1)), axis=-1)\n",
    "print('Feature Map Shape:', feature_maps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-depth",
   "metadata": {},
   "source": [
    "Now we take the following 3x3x2 identity filter $A_{ijk}$:\n",
    "$$\n",
    "A_{:,:,1} = A_{:,:,2} =  \\left( \\begin{array}{ccc}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{array} \\right),\n",
    "$$\n",
    "and apply it to `feature_maps`. This time, we will use a bias of -2 as it produces a nice contrast in the final image. We then pass result through a ReLU, saving the output in variable `outmap`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve to produce output of 2nd CNN layer\n",
    "corner_kernel = np.array([[[0,0,0],\n",
    "                           [0,1,0],\n",
    "                           [0,0,0]],\n",
    "                          [[0,0,0],\n",
    "                           [0,1,0],\n",
    "                           [0,0,0]]])\n",
    "outmap = np.zeros(feature_maps.shape[:2])\n",
    "for i in range(2):\n",
    "    # outmap[:,:] += convolve2d(feature_maps[:,:,i], corner_kernel[:,:,i], mode='same')\n",
    "    outmap[:,:] += correlate2d(feature_maps[:,:,i], corner_kernel[:,:,i], mode='same')\n",
    "bias = -2\n",
    "outmap += bias\n",
    "outmap = relu(outmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-factor",
   "metadata": {},
   "source": [
    "Now that the image has passed through 2 CNN layers.<br>\n",
    "Before visualizing you results, what do expect to see? What would you call the kind of feature represented in our final output feature map? (Think about the input feature maps and what our final CNN layer is doing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot output of 2nd CNN layer\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "ax.imshow(np.clip(outmap, a_min=0, a_max=1.), cmap='gray')\n",
    "ax.set_title('What Might This Feature Be?', {'fontsize': fontsize})\n",
    "ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-genius",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='tfdatasets'></a>\n",
    "\n",
    "## Tensorflow Datasets [^](#contents \"Back to Contents\")\n",
    "<img src='https://3.bp.blogspot.com/-d-nV7xJRmpw/Xo328dcAx3I/AAAAAAAAC7Q/qlqJOle6XIosJ3CGIDJ04F3Voh1iXDg0gCLcBGAsYHQ/s1600/TF_FullColor_Icon.jpg' width='150'>\n",
    "\n",
    "TensorFlow Datasets (TFDS) is a collection of datasets ready to use, with TensorFlow or other Python ML frameworks. These datasets are exposed as `tf.data.Dataset` objects, enabling easy-to-use and high-performance input pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-picture",
   "metadata": {},
   "source": [
    "To improve TensorFlow's performance in our current environment we can run the following commands. Don't fret too much over what all these commands are doing. There are links in the comments if you'd like to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment\n",
    "# that evaluates operations immediately, without building graphs\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/datac_performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure replicable results\n",
    "import os\n",
    "import random as rn\n",
    "SEED = 109\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-fellowship",
   "metadata": {},
   "source": [
    "<div id=\"loadds\" class='exercise'><b>Loading Datasets</b></div></br>\n",
    "\n",
    "TFDS gives us access to dozens of research quality datasets with the simple `tfds.load` method.<br>\n",
    "An extensive catalogue of datasets can be seen <a href='https://www.tensorflow.org/datasets/catalog/overview'>here</a>. You can even <a href='https://www.tensorflow.org/datasets/add_dataset'>write your own custom dataset</a>.\n",
    "\n",
    "### <div style='font-size: 150%'>🐎 or 🧍?</div>\n",
    "\n",
    "But it doesn't mean you should just because you can. The *bizzare* <a href='http://laurencemoroney.com/horses-or-humans-dataset'>Horses or Humans</a> dataset may just be an example of this.<br>\n",
    "Our call to `tfds.load` will use several arguments:\n",
    "- `name`: (str) the dataset to load (you can look these up in the above catalogue)\n",
    "- `split`: (list) some datasets have pre-specified splits; this list defines which splits to load\n",
    "- `shuffle_files`: (bool) files are loaded in random order\n",
    "- `as_supervised`: (bool) loads labels if dataset has them\n",
    "- `with_info`: (bool) also returns an DatasetInfo object with details about the loaded dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318e9e7-ed02-4912-b039-afddc5442348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfds.enable_progress_bar()\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(name=\"horses_or_humans\", split=['train', 'test'],\n",
    "                                         shuffle_files=True, as_supervised=True, with_info=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-donor",
   "metadata": {},
   "source": [
    "The `ds_info` we got from using `with_info=True` gives us a great overview of some facts about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-light",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-insured",
   "metadata": {},
   "source": [
    "<div id=\"dsobj\" class='exercise'><b>The Dataset Object</b></div></br>\n",
    "\n",
    "We'll be working with closely with the `tf.data.Dataset` object so we should learn more about its methods and structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-alabama",
   "metadata": {},
   "source": [
    "**Some Python Arcana: Iterables & Iterators**:<br>\n",
    "\n",
    "The `tf.data.Dataset` object is an *iterable*, which means it implements an `__iter__` method which returns an *iterator* object.<br>\n",
    "An *iterator* is an object that implements a `__next__` method which returns the next element in the iterator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterator from iterable\n",
    "my_iter = iter(ds_train)\n",
    "my_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get next element from iterator\n",
    "next_one = next(my_iter)\n",
    "print(f'Each element in the iterator is of type {type(next_one)} with length {len(next_one)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display element from iterator\n",
    "next_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-remains",
   "metadata": {},
   "source": [
    "Inspecting this tuple we see the 1st element is our image and the 2nd is the lable. Let's visualize the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next_one\n",
    "plt.imshow(image)\n",
    "plt.title(int(label));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-capacity",
   "metadata": {},
   "source": [
    "It appears humans are the positive class. Let's make a dictionary to map class labels to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create human interperatable class names\n",
    "class_names = {0: 'horse', 1: 'human'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-value",
   "metadata": {},
   "source": [
    "**Iterating**\n",
    "\n",
    "Iterables can be looped over with a `for` loop. And while you *can* loop over the `Dataset` iterable object itself it is more common to first use the `as_numpy_iterator()` method if running TF in eager mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 5\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(10,6))\n",
    "for ax, (img, label) in zip(ax.ravel(), ds_train.as_numpy_iterator()):\n",
    "    # break when no more axes left\n",
    "    if ax is None:\n",
    "        break\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[label])\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-deployment",
   "metadata": {},
   "source": [
    "The above method works, but it would be a nuisance to have to always write out all that code just to inspect our datasets.\n",
    "\n",
    "Luckily, `tfds` has a much faster way to do this with the `show_examples()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train examples\n",
    "tfds.show_examples(ds_train, ds_info, rows=rows, cols=cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test examples\n",
    "tfds.show_examples(ds_test, ds_info, rows=rows, cols=cols);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-sector",
   "metadata": {},
   "source": [
    "**Q:** Do you notice anything strange about the test examples? 🤔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-energy",
   "metadata": {},
   "source": [
    "<div id=\"take\" class='exercise'><b>Take, Cardinality, & Batch</b></div></br>\n",
    "\n",
    "**Take**\n",
    "\n",
    "We can use the `take()` method to return a **subset** of the original `Dataset` object of a desired **cardinality**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a subset with cardinality 2\n",
    "my_subset = ds_train.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-dialogue",
   "metadata": {},
   "source": [
    "**Cardinality**\n",
    "\n",
    "It's true that we can check the length of a datset with `len()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way to find a Dataset's length\n",
    "len(my_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-sender",
   "metadata": {},
   "source": [
    "But this is inefficient  for larger datasets.<br>\n",
    "It is preferable to use the `cardinality()` method. This returns a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a better way\n",
    "cardinality = my_subset.cardinality()\n",
    "print(f'Cardinality Type: {type(cardinality)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-modeling",
   "metadata": {},
   "source": [
    "It looks a bit strange when displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying an EagerTensor\n",
    "cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-algorithm",
   "metadata": {},
   "source": [
    "But it behaves just like an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no surprises here!\n",
    "(cardinality + 2) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-toddler",
   "metadata": {},
   "source": [
    "And if you really want to, you can always convert it to a `numpy.int64` object which prints nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-albany",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert to nump.int64\n",
    "cardinality.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-nickel",
   "metadata": {},
   "source": [
    "**Batch**\n",
    "\n",
    "To get the benefits of <a href='https://en.wikipedia.org/wiki/Stochastic_gradient_descent'>**stochastic gradient descent**</a> (SGD) during training, we'd like to feed elements from the dataset into our model in batches.<br>\n",
    "This is handled by the `batch()` method.\n",
    "\n",
    "**Q:** What are some benefits of SGD? 🤔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "num_batch = ds_train.cardinality() / BATCH_SIZE\n",
    "print(f'Number of Potential Batches of size {BATCH_SIZE}:', num_batch.numpy())\n",
    "num_batched_produced = ds_train.batch(BATCH_SIZE).cardinality()\n",
    "print(f'Number of Batches of size {BATCH_SIZE} Produced:', num_batched_produced.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-measurement",
   "metadata": {},
   "source": [
    "**Q:** Why don't these numbers match? What's going on? 🤔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-publicity",
   "metadata": {},
   "source": [
    "The batched dataset is itself a `Dataset`, but now it's an iterable that produces batches.<br>\n",
    "In a **supervised** situation like ours, each batch is a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-characteristic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect first batch\n",
    "my_batch = ds_train.batch(BATCH_SIZE).as_numpy_iterator().next()\n",
    "print(f'Each batch is of type {type(my_batch)} with length {len(my_batch)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-northeast",
   "metadata": {},
   "source": [
    "The 1st element of the tuple are all the images in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch images\n",
    "my_batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-specialist",
   "metadata": {},
   "source": [
    "The 2nd element are all the labels in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch labels\n",
    "my_batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-telephone",
   "metadata": {},
   "source": [
    "**Note:** The batch of images and labels above are both `numpy` arrays. This is because we used `as_numpy_iterator()` on our batched dataset. This is why we were able to use the `shape` attribute. here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-galaxy",
   "metadata": {},
   "source": [
    "We can also strengthen our intuition about the structure of the batched dataset by iterating over the batches and displaying the first image in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first image in each batch\n",
    "fig, axs = plt.subplots(4, 8, figsize=(9,5))\n",
    "axs = axs.ravel()\n",
    "for i, (img_batch, label_batch) in enumerate(ds_train.batch(BATCH_SIZE, drop_remainder=True)):\n",
    "    for (img, label) in zip(img_batch, label_batch):\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].set_title(f'batch {i+1}')\n",
    "        axs[i].axis('off')\n",
    "        break\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-video",
   "metadata": {},
   "source": [
    "<div id=\"cache\" class='exercise'><b>Cache, Prefetch & Shuffle</b></div></br>\n",
    "\n",
    "There are helpful methods we can use to optimize the training process. Most of these descriptions are adapted from the TensorFlow documentation. As always, the documentation is the best place to go if you'd like a deeper understanding.\n",
    "\n",
    "**<a href=\"https://www.tensorflow.org/guide/data_performance#caching\">`Cache`</a>** caches a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch. (perhaps not a good idea for enormous datasets)\n",
    "\n",
    "**`Prefetching`** overlaps the preprocessing and model execution of a training step. While the model is executing training steps, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data.\"\n",
    "\n",
    "**<a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\">`Shuffle`</a>** Randomly shuffles the elements of this dataset.\n",
    "\n",
    "**Note:** cache will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call shuffle after calling cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-celebrity",
   "metadata": {},
   "source": [
    "And of course we can chain all these commands together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = ds_train.cache()\\\n",
    "                         .shuffle(buffer_size=ds_train.cardinality(), seed=SEED, reshuffle_each_iteration=True)\\\n",
    "                         .batch(BATCH_SIZE).prefetch(AUTOTUNE)\\\n",
    "                         .as_numpy_iterator().next() # one batch\n",
    "# show first image in batch\n",
    "plt.imshow(images[0])\n",
    "plt.title(class_names[labels[0]])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-spoke",
   "metadata": {},
   "source": [
    "<div id=\"dspreproc\" class='exercise'><b>Preprocessing with Datasets</b></div></br>\n",
    "\n",
    "Often, we'll want to **preprocess** our data in some way before feeding it into our model.<br>\n",
    "We can use use the `Dataset` object's `map` method to perform arbitrary functions on the elements of the dataset.\n",
    "\n",
    "Because the result of the `map` operation is itself a dataset object, we can continue to chain these commands one after another.<br>\n",
    "Here we are normalizing and resizing our images as part of the preprocessing stage using functions of our own design. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = W = 200\n",
    "\n",
    "def normalize_img(img, label):\n",
    "    return tf.cast(img, tf.float32)/255.0, label\n",
    "\n",
    "def resize_img(img, label):\n",
    "    return tf.image.resize(img, size=[H, W]), label\n",
    "\n",
    "ds_train = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE).map(resize_img, AUTOTUNE)\n",
    "ds_test = ds_test.map(normalize_img, num_parallel_calls=AUTOTUNE).map(resize_img, AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-judge",
   "metadata": {},
   "source": [
    "<div id=\"dataaug\" class='exercise'><b>Data Augmentation</b></div></br>\n",
    "\n",
    "\n",
    "We almost always wish we had *more data*! But it can be expensive and time consuming to gather and label new data.<br>\n",
    "So why not **simulate new data?** We can accomplish this by creating variants of our original data. \n",
    "\n",
    "In the case of images this is very intuitive. Simply rotate your picture of a horse. It's still a horse, but the rotated image is likely different from anything in your data original. As long as the simulated data is not *too* different from the sort of example's we'd like to learn, this can help our model generalize better to previously unseen examples not in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "def random_rotate(image, label):\n",
    "    \"\"\"Dataset pipe that rotates an image, helper function to augment below\"\"\"\n",
    "    shape = image.shape\n",
    "    deg = tf.random.uniform([],-10.,10.)\n",
    "    image = tfa.image.rotate(image, deg/180.*np.pi, interpolation=\"BILINEAR\")\n",
    "    image.set_shape((shape))\n",
    "    label.set_shape(())\n",
    "    return image, label\n",
    "\n",
    "def random_zoom(image, label):\n",
    "    \"\"\"Dataset pipe that zooms an image, helper function to augment below\"\"\"\n",
    "    rand_float = tf.random.uniform([],10,20)\n",
    "    rand_int = tf.cast(rand_float, tf.int32)\n",
    "    image = tf.image.resize_with_crop_or_pad(image,\n",
    "                                             H + H//rand_int,\n",
    "                                             W + W//rand_int)\n",
    "    image = tf.image.random_crop(image, size=[H, W, 3])\n",
    "    return image, label\n",
    "    \n",
    "def augment(image, label):\n",
    "    \"\"\"Function that randomly alters an image with\n",
    "       flipping, rotation, zoom, and contrast adjustment\"\"\"\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image, label = random_rotate(image, label)\n",
    "    image, label = random_zoom(image, label)\n",
    "    image = tf.image.random_contrast(image, lower=.95, upper=1.)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-preservation",
   "metadata": {},
   "source": [
    "Here are just a few examples created using the augmentation functions defined above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a batch of altered images\n",
    "fig, axs = plt.subplots(4,4, figsize=(6,6))\n",
    "aug_batch = ds_train.map(augment, num_parallel_calls=AUTOTUNE).take(16)\n",
    "for ax, (img, label) in zip(axs.ravel(), aug_batch):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-spain",
   "metadata": {},
   "source": [
    "Data augmentation is an important topic and it will be revisited several times throughout the course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-california",
   "metadata": {},
   "source": [
    "<div id=\"val_split\" class='exercise'><b>Creating a Validation Set</b></div></br>\n",
    "\n",
    "We can see from `ds_info` above that this dataset had predefined train and test sets. We loaded both.\n",
    "\n",
    "Some TF datasets also have a validation set. Others have only train. We'd like to use a validation set while training our model. And in situations where your data set only has a train set it will be important to know how to create new splits. Strangely, this seems to be one functionality that `tfds` does not provide by default. We can write the code ourselves but it is rather inscrutable. You can read more about this solution [here](https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test).\n",
    "\n",
    "You won't be required to make splits from TF datasets in this section or in the HW. But you may wish to learn more about it on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inscrutable splitting code; not for the feint of heart!\n",
    "# 9 train samples for every validation sample\n",
    "train_per_val = 9\n",
    "\n",
    "def mapping(*ds):\n",
    "    return ds[0] if len(ds) == 1 else tf.data.Dataset.zip(ds)\n",
    "\n",
    "ds_val_example = ds_train.skip(train_per_val).window(1, train_per_val + 1).flat_map(mapping)\n",
    "ds_train_example = ds_train.window(train_per_val, train_per_val + 1).flat_map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-workshop",
   "metadata": {},
   "source": [
    "This method breaks cardinality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative cardinality?! Preposterous!\n",
    "ds_val_example.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-cylinder",
   "metadata": {},
   "source": [
    "But slowly counting the elements in each split shows that the original dataset was indeed split correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct sizes displayed by the slow method\n",
    "original_size = ds_train.cardinality()\n",
    "val_size = len(list(ds_val_example.as_numpy_iterator()))\n",
    "train_size = len(list(ds_train_example.as_numpy_iterator()))\n",
    "print(f'Original Size: {original_size.numpy()}')\n",
    "print(f'Train Split Size: {train_size}')\n",
    "print(f'Val Split Size: {val_size}')\n",
    "print(f'Original Size = Train Split + Val Split: {(original_size == val_size + train_size).numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-catch",
   "metadata": {},
   "source": [
    "Perhaps you can devise a better method!"
   ]
  },
  {
   "attachments": {
    "ec1d1aa0-3e29-4734-9171-fa26d58ed5cb.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "judicial-lightweight",
   "metadata": {},
   "source": [
    "<a id='keras'></a>\n",
    "## Keras [^](#contents \"Back to Contents\")\n",
    "![image.png](attachment:ec1d1aa0-3e29-4734-9171-fa26d58ed5cb.png)\n",
    "\n",
    "From our work in the previous sections of this notebook I'm sure you can now appreciate that implementing a non-trivial CNN by hand would be a pain, and the looping code above was far from optimized. So we will be using <a href='https://keras.io/'>Keras</a> to quickly construct our neural networks.\n",
    "\n",
    "The Keras API sits on top of Tensorflow. It allows user to work at a more intuitive level of abstraction where the basic objects are **layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-liquid",
   "metadata": {},
   "source": [
    "<div id=\"keras_layers\" class='exercise'><b>Layers of a CNN in Keras</b></div></br>\n",
    "<img src='fig/cnn1.png' width='900px'>\n",
    "\n",
    "The following is a list of layers commonly used when building CNNs with Keras.<br>\n",
    "A link to the official documentation for each layer is also provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-tender",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "[**tf.keras.Input**](https://www.tensorflow.org/api_docs/python/tf/keras/Input)(\n",
    "    shape=None, **kwargs\n",
    ")\n",
    "\n",
    "<div style='color:red'><strong>The input is not a layer!</strong></div>\n",
    "\n",
    "As Pavlos said in lecture, you shouldn't think of the input to your network as a layer. Unfortunately, in Keras, most components of a network are referred to as 'layers'. Someone must have come to their senses because now `Input` can be found in the base `tf.keras` module. While it *can* still be imported from `tf.keras.layers`, we are civilized people and shall speak no more of that.\n",
    "\n",
    "The network will be expecting input of fixed shape which must be specified with the `shape` parameter. You should look at the data you are using to determine this shape.\n",
    "\n",
    "Adding an explicit `Input` object to your layer is not required as most layers have an `input_shape` that can be specified if they are the first layer in the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-oriental",
   "metadata": {},
   "source": [
    "### 2D Convolutional Layers\n",
    "\n",
    "[**keras.layers.Conv2D**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) (filters, kernel_size, strides=(1, 1), padding='valid', activation=None, use_bias=True, \n",
    "                    kernel_initializer='glorot_uniform', data_format='channels_last', \n",
    "                    bias_initializer='zeros')\n",
    "\n",
    "<img src='fig/conv-many-filters.png' width='550px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-formula",
   "metadata": {},
   "source": [
    "**Some quick review if skipping to this section:**\n",
    "\n",
    "A convolutional layer is composed of **filters**, which are composed of **kernels** which are themselves composed of **weights**. Each filter also has a bias term though it is often not depicted in diagrams (it is exluded in the one above for example). We learn the weights and biases from our data. Each conv layer also has an associated **activation function** such as ReLU or sigmoid. \n",
    "\n",
    "The **number of filters** and the **height and width of the kernels** of which they consist are set by the `filters` and `kernel_size` (a tuple) arguments respectively. \n",
    "\n",
    "The **depth of the filters is fixed** by the depth (i.e., 'channels' or 'filter maps') of the input to the conv layer. \n",
    "\n",
    "The output of the conv layer is a 3D tensor which is a set of **feature maps**. Each feature map is itself the output of one of the layer's filters convolving on the input. The height and width of the feature map tensor is determined by the input size, `kernel_size`, `padding`, and `stride`. The depth of the output tensor (i.e, number of feature maps) is equal to the number of filters in the layer.\n",
    "                    \n",
    "Keras also has a [1D convolutional layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) used for time series data and a [3D convolutional layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv3D) used for video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-locking",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "[**keras.layers.MaxPool2D**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "\n",
    "<img src='fig/maxpool.png' alt='MaxPool' width='400px'>\n",
    "\n",
    "Pooling layers are also comprised of filters and feature maps. Let's say the pooling layer has a 2x2 receptive field and a stride of 2. This stride results in feature maps that are one half the size of the input feature maps. We can use a max() operation for each receptive field. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-polyester",
   "metadata": {},
   "source": [
    "### Dropout Layers\n",
    "<img src='fig/dropout.gif' width='200px'>\n",
    "\n",
    "[**tf.keras.layers.Dropout**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)(rate, seed=None)\n",
    "\n",
    "Dropout consists in randomly setting a fraction of input units to 0 at each update during training time. In Keras this fraction is set by the `rate` parameter. At inference time, trained weights are multipled by $(1 - \\text{rate})$. Dropout often used to help prevent overfitting by limiting the complexity of our model. It can also prevent groups of neurons from 'conspiring' together to have a large affect on the output, something traditional forms of weight regularization would not catch.\n",
    "\n",
    "**Caution:** Dropout's behavior is not the same if performed after a convolutional layer! [See this post for more information](https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2).\n",
    "\n",
    "**Q:** Why might it make sense to think of dropout as a type of ensemble method? 🤔\n",
    "\n",
    "References<br>\n",
    "[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-demonstration",
   "metadata": {},
   "source": [
    "### Flatten Layers\n",
    "\n",
    "\n",
    "[**keras.layers.Flatten**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)()\n",
    "\n",
    "Like `Input` and `Dropout`, `Flatten` is not a layer in the traditional sense. It has no learned parameters and no parameters other than `input_shape`. Its only function is to flatten its multi-dimensional input into a flat vector. The flatten layer sits between our final 2D output (either from Conv2D or MaxPool2D) and our first fully connected, `Dense` layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-comparison",
   "metadata": {},
   "source": [
    "### Fully Connected Layers.\n",
    "\n",
    "[**keras.layers.Dense**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)(units, activation=None, use_bias=True, \n",
    "                    kernel_initializer='glorot_uniform', bias_initializer='zeros')\n",
    "<img src='fig/dense.png' width='250px'>\n",
    "\n",
    "Most CNNs have one or more dense layers at the end with the final layer referred to as the **output layer**. You'll need to specify the number of `units` in each layer (sometimes called 'neurons' or 'nodes') as well as the `activation`. \n",
    "\n",
    "*Special care should be taken in deciding on the activation function for the output layer!* The correct choice of activation in this final layer depends on the task we are training our model to perform. For example, a linear activation for regression, but a sigmoid for binary classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-adoption",
   "metadata": {},
   "source": [
    "<div id=\"keras_model\" class='exercise'><b>Defining a Model</b></div></br>\n",
    "\n",
    "Here our task is building a CNN to (surprise!) classify images as either horses are humans. 🤖-- 🐎 or 🧍?\n",
    "\n",
    "You be using the [Keras sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) to create your CNN model and define its architecture. There are two ways to do this.<br>\n",
    "You can create a model object using the `Sequential` constructor while:\n",
    "1. passing it a list of parameterized layers\n",
    "2. passing no layers and then using the newly created model's `add()` method to attach layers to your model.\n",
    "\n",
    "We will begin with a very simply CNN. It should consist of the following layers:\n",
    "- **Input** /w input shape derived below\n",
    "- **Conv2D** /w 32 filters, 3x3 kernals, default padding & stride, and relu activation\n",
    "- **MaxPool2D** of size 3x3\n",
    "- **Conv2D** /w 64 filters, 3x3 kernals, default padding & stride, and relu activation\n",
    "- **MaxPool2D** of size 3x3\n",
    "- **Conv2D** /w 128 filters, 3x3 kernals, default padding & stride, and relu activation\n",
    "- **MaxPool2D** of size 2x2\n",
    "- **Flatten**\n",
    "- **Dense** /w 64 units and relu activation\n",
    "- **Dense** /w 1 **?** units and **?** for an activation\n",
    "\n",
    "\n",
    "If you find these instructions a bit vague it is because you are being gently nudged to both look at all the helpful **documentation** linked above a think a bit about the task you are giving your network. 😉 \n",
    "\n",
    "Please continue on until you reach the **end of exercise** note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPool2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-kenya",
   "metadata": {},
   "source": [
    "We'll need to specify an input shape for our model. We can derive this by looking at one of the images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find input shape\n",
    "for element in ds_train.take(1).as_numpy_iterator():\n",
    "    image_shape = element[0].shape\n",
    "    print(f'The input shape of each batch is: {BATCH_SIZE}')\n",
    "    print(f'The input shape of each image is: {image_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a CNN\n",
    "inputs = Input(image_shape)\n",
    "# x = Conv2D(32, 3, padding='valid', activation=\"relu\")(inputs)\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides=(1, 1), padding='valid', \n",
    "       activation='relu', use_bias=True, \n",
    "       kernel_initializer='glorot_uniform', \n",
    "       data_format='channels_last', \n",
    "       bias_initializer='zeros')(inputs)\n",
    "\n",
    "# x = MaxPool2D((3,3))(x)\n",
    "x = MaxPool2D(pool_size=(3, 3), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "# x = Conv2D(64, 3, padding='valid', activation=\"relu\")(x)\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides=(1, 1), padding='valid', \n",
    "       activation='relu', use_bias=True, \n",
    "       kernel_initializer='glorot_uniform', \n",
    "       data_format='channels_last', \n",
    "       bias_initializer='zeros')(x)\n",
    "\n",
    "# x = MaxPool2D((3,3))(x)\n",
    "x = MaxPool2D(pool_size=(3, 3), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "# x = Conv2D(128, 3, padding='valid', activation=\"relu\")(x)\n",
    "x = Conv2D(filters = 128, kernel_size = 3, strides=(1, 1), padding='valid', \n",
    "       activation='relu', use_bias=True, \n",
    "       kernel_initializer='glorot_uniform', \n",
    "       data_format='channels_last', \n",
    "       bias_initializer='zeros')(x)\n",
    "\n",
    "# x = MaxPool2D((3,3))(x)\n",
    "x = MaxPool2D(pool_size=(3, 3), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model1 = Model(inputs=inputs, outputs=outputs, name=\"Model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-console",
   "metadata": {},
   "source": [
    "Now let's inspect the model with the `summary()` method. Notice how the dimensions change as the image passes through different stages of the network. Also take note of which layers have the most parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-wallet",
   "metadata": {},
   "source": [
    "**Compiling the Model**\n",
    "\n",
    "Before we can train our model Keras requires use its `compile()` method to specify a few things:\n",
    "- An [optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) which controls how weights are updated\n",
    "- A [loss function](https://www.tensorflow.org/api_docs/python/tf/keras/losses) which your model is trying to minimize\n",
    "- A list of [metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) (optional) which are other functions that can be monitored during training\n",
    "\n",
    "The compile method accepts optimizers, losses, and elements of the metrics list as either as objects from their respective Keras modules (imported below) or their names as strings (with underscores replacing spaces).\n",
    "\n",
    "Compile your model with:\n",
    "- `optimizer='SGD'`\n",
    "- `meterics = ['acc']` (e.g., accuracy)\n",
    "- `loss = `? (think about the task and look at the documentation)\n",
    "\n",
    "**Note:** Remember that `metrics` is a list.\n",
    "\n",
    "**Hint:** You can also use Jupyter's `Tab` aoutcomplete on the `losses` module to look at your options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile your model\n",
    "# your code here\n",
    "model1.compile(optimizer = 'Adam',\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-being",
   "metadata": {},
   "source": [
    "<div id=\"keras_train\" class='exercise'><b>Train the Model</b></div></br>\n",
    "\n",
    "Train your model by calling its `fit` method. We'll pass in our training data, validation data, and the number of epochs. Note the chaining of methods on the dataset objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-message",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model1.fit(ds_train.cache()\\\n",
    "                    .shuffle(buffer_size=ds_train.cardinality(), seed=SEED, reshuffle_each_iteration=True)\\\n",
    "                    .batch(BATCH_SIZE).prefetch(AUTOTUNE),\n",
    "                    validation_data=ds_test.cache()\\\n",
    "                    .shuffle(buffer_size=ds_test.cardinality(), seed=SEED, reshuffle_each_iteration=True)\\\n",
    "                    .batch(BATCH_SIZE).prefetch(AUTOTUNE),\n",
    "                    epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-cardiff",
   "metadata": {},
   "source": [
    "<div class='exercise' id='plottinghistory'><b>Plot the Training History</b></div></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-burning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to avoid repeated code later\n",
    "def plot_loss(model_history, out_file = None):\n",
    "    \"\"\"\n",
    "    This helper function plots the NN model accuracy and loss.\n",
    "    Arguments:\n",
    "        model_history: the model history return from fit()\n",
    "        out_file: the (optional) path to save the image file to.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n",
    "    \n",
    "    history = model_history\n",
    "    \n",
    "    ax[0].plot(history.history['acc'])\n",
    "    ax[0].plot(history.history['val_acc'])\n",
    "    ax[0].set_title('model accuracy')\n",
    "    ax[0].set_ylabel('accuracy')\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].legend(['train', 'validation'], loc='upper left')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    ax[1].plot(history.history['loss'])\n",
    "    ax[1].plot(history.history['val_loss'])\n",
    "    ax[1].set_title('model loss')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    ax[1].legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    if out_file:\n",
    "        plt.savefig(out_file)\n",
    "\n",
    "plot_loss(model_history = history);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-tulsa",
   "metadata": {},
   "source": [
    "<div id=\"keras_eval\" class='exercise'><b>Evaluate the Model</b></div></br>\n",
    "\n",
    "Let's see how we did using the model's `evaluate` method.\n",
    "\n",
    "**Note:** Your model will complain if unless you passing it batches of the same size it saw during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-retailer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.evaluate(ds_test.batch(BATCH_SIZE).prefetch(AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f348c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(ds_train.batch(BATCH_SIZE).prefetch(AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f3f3c8",
   "metadata": {},
   "source": [
    "<div id=\"\" class='exercise'><b>Saving Your Model</b></div></br>\n",
    "\n",
    "It would be nice if we could save and restore our models between sessions. Here's a few methods!\n",
    "\n",
    "First, we can save just the learned weights. Careful though! You will have to first reconstruct the same architecture to load these weights into. This also does not save the optimizer state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_weights(\"data/models/model1_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce62a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_weights(\"data/models/model1_weights.ckpt\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae606d",
   "metadata": {},
   "source": [
    "Alternatively, we can save the entire model, including the architecture itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19782386",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model1.save('data/models/model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30094e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.models.load_model('data/models/model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm reloading worked\n",
    "model1.evaluate(ds_train.batch(BATCH_SIZE).prefetch(AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a12e1",
   "metadata": {},
   "source": [
    "<div id=\"callbacks\" class='exercise'><b>Keras Callbacks</b></div></br>\n",
    "\n",
    "Training CNNs can take a long time. We should checkpoint our model so we don't lose progress and stop early if we don't see improvement to help save on training time. Checkpointing also allows us to recover our best performing version of te model from any point during the training process as later version have a tendency to be overfit.</br>\n",
    "\n",
    "[Keras Callbacks Documentation](https://keras.io/api/callbacks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-trash",
   "metadata": {},
   "source": [
    "Unfortunately, subroutines of many callbacks currently result in some [ugly (but harmless) warnings](https://github.com/tensorflow/tensorflow/issues/44178). Until the leves at the Tensorflow workshop put things right we can simply silence them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "# Point the keras.models.load() method at the path given below to reload the checkpointed model\n",
    "mc = ModelCheckpoint('data/models/checkpoints', monitor='val_loss', save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-tract",
   "metadata": {},
   "source": [
    "Callbacks are used by passing them as a list to a model's `fit()` method.<br>\n",
    "Example: `model.fit(...., callbacks=[es, mc])`\n",
    "\n",
    "**Note**: If your model trains very quickly then checkpointing may not be worth it. The time it takes to write to disk after each epoch can sometimes take longer than the training itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-affairs",
   "metadata": {},
   "source": [
    "<div id=\"improving\" class='exercise'><b>Exercise: Improving on Baseline Model</b></div></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-arthur",
   "metadata": {},
   "source": [
    "Now see if you can improve on the performance of you previous model. You can try:\n",
    "- adjusting the architecture\n",
    "    - more/fewer layers/filters/units\n",
    "    - adding a dropout(s) layer\n",
    "- adding data augmentation\n",
    "    - tweaking the various transformations included\n",
    "- using callbacks like checkpointing or early stopping\n",
    "- adjust the number of epochs\n",
    "- adjust the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-lancaster",
   "metadata": {},
   "source": [
    "**Note:** What follows is not the 'solution,' but merely one way you could reparameterize your model in an attempt to improve the evaluation on the validation set. Can you get below a validation loss of ~3.4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GaussianNoise, BatchNormalization, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try and improve on your first CNN\n",
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "# end your code here\n",
    "model2 = Model(inputs=inputs, outputs=outputs, name=\"Model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'adam',\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-chess",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# CUSTOMIZE THESE PARAMETERS\n",
    "BATCH_SIZE2 = 32\n",
    "history2 = model2.fit(ds_train.cache()\\\n",
    "                    .shuffle(buffer_size=ds_train.cardinality(), seed=SEED, reshuffle_each_iteration=True)\\\n",
    "                    .map(augment, num_parallel_calls=AUTOTUNE)\\\n",
    "                    .batch(BATCH_SIZE2).prefetch(AUTOTUNE),\n",
    "                    validation_data=ds_test.cache()\\\n",
    "                    .shuffle(buffer_size=ds_test.cardinality(), seed=SEED, reshuffle_each_iteration=True)\\\n",
    "                    .batch(BATCH_SIZE2).prefetch(AUTOTUNE),\n",
    "                    epochs=50,\n",
    "                    callbacks=[es],\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-rogers",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(ds_test.batch(BATCH_SIZE).prefetch(AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(ds_train.batch(BATCH_SIZE).prefetch(AUTOTUNE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs109b",
   "language": "python",
   "name": "cs109b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
